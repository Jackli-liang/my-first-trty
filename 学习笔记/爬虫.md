## python爬虫基础

### 1 requestes库

1.1 request模块的安装

pip install requests

支持的http方法

1 GET 当客户端向web服务器请求一个资源的时候使用，它被用来访问静态资源，比如HTML文档和图片（提供请求的入口即请求的url）

2 HEAD 当客户端向web服务器请求一个资源的一些信息而不是资源的全部信息的时候使用

主要用于确认URL的有效性以及资源更新的日期时间等

3 POST 当客户端向服务端发送信息或者数据的时候使用，表单提交或发生大量复杂的数据（提供提交的数据用data参数保存）

4 PUT 当客户端向web服务器端指定URl发生一个替换的文档或者上传一个新的文档的时候使用

5 DELETE 当客户端尝试从web服务器端删除一个由请求URL唯一标识的文档的时候使用

6 OPTIONS 当客户端想要决定其他可用的方法来检索或者处理web服务端的一个文档时使用

7 CONNECT当客户端想要确定一个明确的连接到远程主机的时候使用，通常便于通过Http代理服务器进行SSL加密通信（Https）连接使用

reuqests-----证书校验

- 关闭证书校验----------verify=False，否则SSLerror

requests------网络问题     ConnectionError异常

requests------状态码        HttpError 异常

requests-----请求超时     TimeOur  异常

requests------重定向次数过多    TooManyRedirects异常

1.2 url构造方法  

```yaml
import requests

data = {
    'name':'imooc'
    } 
response = requests.get('http://httpbin.org/get,params=data)
print(response.url)

##### http://httpbin.org/get?name=imooc
```

1.3 响应状态码  status_code

| 1xx  | **指示信息------表示请求已接受，继续处理**    |
| ---- | --------------------------------------------- |
| 2xx  | 成功-------表示请求已被成功接受、理解、接受   |
| 3xx  | 重定向-----信息不完整需要进一步补充           |
| 4xx  | 客户端错误-------请求有语法错误或请求无法实现 |
| 5xx  | 服务器端错误-------服务器未能实现合法的请求   |

1.4 请求超时

设置关键字  timeout=0.1  在0.1s内没有响应，终端请求抛出timeout错误

1.5 cookies

cookies存在响应体response当中，和构造url相同，构造玩之后用get方法请求

1.6 保持登录session 

1.7 设置代理

```
设置代理格式

proxy_dict = {
    'http':'http://用户名:密码@代理地址:代理端口',   # 带密码
    'https':'https://代理地址:代理端口'
}
```

### 2 xpath库

2.1 路径表达式 

| 路径表达式             | 描述                                              |
| ---------------------- | ------------------------------------------------- |
| /div                   | 从根节点开始选取div节点                           |
| //a                    | 选取文档中所有a节点而不考虑其位置                 |
| @class                 | 选取名为class的属性，选取节点的属性需要[]         |
| .                      | 选取当前节点                                      |
| ..                     | 选取当前节点的父节点                              |
| /div/a                 | 从根节点开始选取div节点下的a节点                  |
| /div/a[2]/img          | 从根节点开始选取div节点下的第二个a节点下的Img节点 |
| //div[@class='header'] | 选取所有属性class的值为header的div节点            |
| /*                     | 选取文档中所有元素                                |

- 在节点后加上带有数字的方括号，可以根据兄弟节点在文档中出现的先后次序选择元素（数字为第一个依次类推--------索引）
- 通过在节点后加上带有属性名----值对的方括号，可以筛选出需要的节点
- 属性值的写法为：@属性名=‘属性值’

| xpath函数                      | 描述                                         |
| ------------------------------ | -------------------------------------------- |
| /div/p/text()                  | 选取p节点的文本内容                          |
| //div[contains(@class,'post')] | 选取带有class属性且包含'post'的所有div节点， |
| /div/p[last()-1]               | 选取div下倒数第二个p节点                     |
| /div/p[postion()>1]            | 选取div下第二个p节点后的所有兄弟节点         |
| /div/a\|div/p                  | 选取div下的a节点和div下的p节点               |
|                                |                                              |

### 3 lxml库

- lxml是一种使用python编写的库，可以迅速灵活地处理XML
- 支持XPath，利用XPath语法，快速定位元素以及节点信息
- 从lxml导入etree,利用etree.HTML补充转化为html，在利用etree.tostring转为字符串

3.1 利用lxml库处理html数据

------

### 4 Beautiful Soup库

4.1 创建BeautifulSoup对象

```yaml
from bs4 import BeautifulSoup

# 根据HTML网页字符串创建BeautifulSoup对象
soup = BeautifulSoup(
		html_doc,               #html文档字符串
		'html.parser'或'lxml'   #html解析器
		encoding='utf-8'		#html文档的编码方式
)

# 方法find_all(name，attrs，string)----节点名称，节点属性，节点内容

# 查找所有标签为a的节点
soup.find_all('a')

# 查找所有标签为a，链接符合/viwe/123.htm形式的节点
soup.find_all('a',href='/view/123.htm')
soup.find_all('a',href=re.compile(r'/view/\d+\.htm'))

#查找所有标签为div，class为abc，文字为Python的节点
soup.find_all('div',class_='abc',string='Python')
---------------------------------------
# 得到节点: <a href='1.html'>Python</a>

#获取查找到的节点的标签名称
node.name

#获取查找到的a节点的href属性
node['href']

#获取查找到的a节点的链接文字
node.get_text()

```

### 5 selenium库

什么是selenium库

- selenium是一个用于web应用程序自动化测试工具，selenium测试直接运行在浏览器中
- 像真正的用户在操作一样，驱动浏览器执行特定的动作，如点击、下拉等

selenium支持的浏览器：IE（7891011），firefox，safari，chrome，opera

selenium在爬虫中的应用

- 获取动态网页中的数据，一些动态的数据我们在获取的源码中并没有显示的这一类动态加载数据
- 用于模拟登陆

5.1 selenium库的基础

```yaml
import time

from selenium import webdriver

browser = webdriver.Chrome()

browser.get('http://www.baidu.com')
# 找到百度的所搜索框，发送python
browser.find_element_by_xpath('//input[@id="kw"]').send_keys('python')
browser.find_element_by_xpath('//input[@id="su"]').click()
time.sleep(3)
# 网页标题
print(browser.title)
# 网页源代码
print(browser.page_source)
# 网页cookies
print(browser.get_cookies())

browser.quit()
```

------

5.2 selenium网页定位技巧

find_element_by_id()

find_element_by_name()

find_element_by_class_name()

find_element_by_tag_name()

find_element_by_link_text()

find_element_by_partial_link_text()

find_element_by_xpath()

find_element_by_css_selector()

5.3 selenium---网页交互之页面刷新与切换

- 设置浏览器窗口大小，但会压缩网页元素位置----test_webdriver.set_window_size(480,800) 
- 控制浏览器后退 driver.back()
- 控制浏览器前进 driver.forward() 
- 控制浏览器刷新 driver.refresh()

5.4 网页交互之响应鼠标事件

导入库  from selenium.webdriver.common.action_chains import ActionChains

perform():执行所有ActionChains中存储的行为

context_click():右击

double_click():双击

drag_and_drop():拖动

move_to_element():鼠标悬停

```yaml
import time

from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains

# 得到Chrome浏览器的驱动
browser = webdriver.Chrome()
# 调用Chrome去访问网址
browser.get('http://www.qq.com')
# 设置浏览器窗口最大化
browser.maximize_window()
# xpath方法找到‘网页’这个标签
pos = browser.find_element_by_xpath('//*[@id="searchSelected"]')
time.sleep(2)
# 得到驱动，控制鼠标移动到标签上，注意perform()是执行这一操作的
ActionChains(browser).move_to_element(pos).perform()
time.sleep(5)

# 退出浏览器
browser.quit()
```

------

5.5 selenium设置元素等待

（1）显式等待：

​	明确的要等到某个元素的出现或者是某个元素的可点击的条件，等不到就一直等，除非在规定的时间之内都没找到。那么就跳出Exception

导入包  form selenium.webdriver.common.by import By----------简写用包

​	     from selenium.webdriver.support.ui import WebDriverWait--------等待用包

​	     from selenium.webdriver.support import expected_conditions as EC---场景判断，判断某个元素是否出现

```yaml
from selenium import webdriver
# 简写用包
from selenium.webdriver.common.by import By
# 等待用包
from selenium.webdriver.support.ui import WebDriverWait
# 场景判断，用来判断某个元素是否出现
 from selenium.webdriver.support import expected_conditions as EC
 
 test_driver = webdriver.Chrome()
 test_driver.maximize_window()
 tets_driver.get('http://www.baidu.com')
 # WebDriverWait设置显示等待
 #1、test_driver 2、timeout 3、0.5s轮询一次
 # EC场景判断，通过id来找相关的元素  kw
 element = WebDriverWait(test_driver,5,0.5).until(EC.presence_of_element_located((By.ID,'kw')))
 element.send_keys('python’)
 time.sleep(2)
 
 test_driver.quit()
```

(2) 隐式等待：

​	当脚本执行到某个元素定位时，如果元素可以定位，则继续执行；如果元素找不到，则它将以轮询的方式不断地判断元素是否被定位到

```yaml
from selenium import webdriver
from selenium.common.expections import NoSuchElementExpection

test_driver = webdriver.Chrome()
test_driver.implicitly_wait(5)
test_driver.get('http://www.baidu.com')
try:
	test_driver.find_element_by_id('kw').send_keys('python')
	test_dricer.find_element_by_xpath('//input[@id="su"]').click()
	time.sleep(3)
expect NoSuchElementExpection as e:
	print(e)
test_driver.quit()
```

------

### 6 pyquery库的用法详解

6.1 初始化方式	

```yaml
html = '''
<div>
    <ul>
         <li class="item-0">first item</li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-0 active"><a href="link3.html"><span class="bold">third item</span></a></li>
         <li class="item-1 active"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a></li>
     </ul>
 </div>
'''

from pyquery import PyQuery as pq
doc = pq(*)
# *既可以是字符串,也可以是url,也可以是文件
print(doc("li"))
#1 用css选择器来实现，如果要选id前面加#，如果选class，前面加.，如果选标签名，什么也不加
doc = pq(url='http://www.baidu.com')
print(doc('head'))#返回head标签
#2 程序会自动请求url
doc = pq(filename='D://demo.html')
print(doc("li"))
#3 直接传入文件名称及路径，程序会自动寻找并请求
```

6.2 基本css选择器

```yaml
from pyquery import PyQuery as pq
doc = pq(html)
items = doc('.list') # 拿到items

lis = items.find('li')
# 利用find方法，查找items里面的li标签，得到的lis也可以继续调用find方法往下查找，层层剥离

lis = items.children('.active') 
# 也可以用.children()查找直接子元素

container = items.parent()
# parent()查找对象的父元素

parents = items.parents() # parents（）祖先节点
parent = items.parents('.wrap') # 当然也可以传入参数

print(li.siblings())
#.siblings()兄弟元素，即同级别的元素，不包括自己
```

6.3 遍历

```yaml
from pyquery import PyQuery as pq
doc = pq(html)
lis = doc("li").items  # .items会是一个生成器
for li in lis:
	print(li)
```

6.4 获取属性

```yaml
from pyquery import PyQuery as pq
doc = pq(html)
a = doc('.item-0.active a')
print(a.attr("href"))
print(a.attr.href)
# 以上两种方法都可以实现
# 定义a标签的href属性用于指定超链接目标的URL。 如果用户选择了a标签中的内容，那么浏览器会尝试检索并显示href属性指定的URL所表示的文档，或者执行JavaScript表达式、方法和函数的列表
```

6.5 DOM操作

```yaml
from pyquery import PyQuery as pq
doc = pq(html)
li = doc('.item-0.active')

li.remove_class("active) # 删除标签的某一个属性
li.add_Class('active') # 增加标签的某一个属性

li.attr('name', 'link') # 增加一个属性
li.css('font-size', '14px') # 增加一个css
```

6.6 伪类选择器

```
from pyquery import PyQuery as pq
doc = pq(html)
li = doc('li:first-child')
print(li)
li = doc('li:last-child')
print(li)
li = doc('li:nth-child(2)')
print(li)
li = doc('li:gt(2)')
print(li)
li = doc('li:nth-child(2n)')
print(li)
li = doc('li:contains(second)')
```



------

### 7 实战爬取51job

------

## 异步爬虫之Scrapy

### 1 scrapy的基本介绍

1.1 scrapy的安装

1.2 scrapy框架的介绍,组件,数据流

​	scrapy engine(引擎):用来处理整个系统的数据流处理,触发事务(框架核心)

​	scheduler(调度器):用来接收引擎发过来的请求,压入队列中,并在引擎再次请求的时候返回.可以想象成一个URL的优先队列,由它来决定下一个要抓取的网址是什么.同时去除重复的网址

​	downloader(下载器):用于下载网页内容,并将网页内容返回给spiders

​	spiders(蜘蛛):爬虫是主要干活的,用于从特定的网页中提取自己需要的信息,即所谓的实体Item,用户也可以从中提取出链接,让scrapy继续抓取下一个页面

​	pipeline(数据管道):负责处理爬虫从网页中抽取的实体,主要的功能是持久化实体,验证实体的有效性,清除不需要的信息.当页面被爬虫解析后,将被发送到项目管道,并经过几个特定的次序处理数据

​	下载器中间件:它是按照优先级被调用的;当reuqests从引擎向下载器传递时,数字小的下载器中间件先执行,数字大的后执行;当下载器将response向引擎传递,数字大的下载器中间件先执行,小的后执行.

​	下载器中间件是个类,类里面可以定义方法,例如:process_request(),process_response(),process_exception()

​	爬虫中间件:1 处理引擎传递给爬虫的响应 2 处理爬虫传递给引擎的请求 3 处理爬虫传递给引擎的引擎项

​	管道: 每个管道的组件都是实现了某个功能的python类,常见的功能有清理html数据,做确认,查重,存入数据库

解析:引擎首先会将爬虫文件中的起始url获取,并提交到调度器中.如果需要从url中下载数据,则调度器会将url通过引擎提交给下载器,下载器根据url去下载指定内容(响应体).下载好的数据会通过引擎移交给爬虫文件,爬虫文件可以将下载数据指定格式的解析.如果解析出的数据需要进行持久化的存储,则爬虫文件会将解析好的数据通过引擎交给管道进行持久化的存储.

------

### 2 建立scrapy项目

2.1 建立项目

1 scrapy startproject + 项目名称(tubatu_scrapy)

​	包含spiders(蜘蛛目录),spider.cfg(项目主配置),其他的.py文件

2 在进入spiders目录之后,执行  scrapy genspider tubatu + 目标域名

​	此时在tubatu.py中会生成TubatuSpider得类,他有name='tubatu'这个属性

3 执行爬虫程序	

```yaml
# 项目的根目录下执行 scrapy shell + 网址名进入终端

将目录切换到spiders的项目目录下,执行scrapy crawl  + name 或者
	进入到spiders文件夹里面  scrapy runspider doutula.py

​	也可以在spiders目录同级的目录中创建main.py

​	from scrapy import cmdline

​	cmdline.execute('scrapy crawl tubatu'.split())

​	然后run运行项目

​	在tubatu.py的parse函数方法写具体的爬虫代码

​	爬取之后,指定格式进行存储:
# 在项目的根目录
scrapy crawl qiubai -o qiubai.json
scrapy crawl qiubai -o qiubai.xml
scrapy crawl qiubai -o qiubai.csv

如果需要持久化存储,则将解析到的数据封装到item对象中
然后yield item,注意items.py中的字段名
```